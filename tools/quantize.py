#!/usr/bin/env python3
"""
SpeyTech Model Quantizer
Convert PyTorch model weights to Q16.16 fixed-point C headers

Usage:
    python quantize.py model.pth layer_name output_dir

Author: William Murray
Copyright (c) 2026 The Murray Family Innovation Trust
License: GPL-3.0 or Commercial
"""

import sys
import argparse
from pathlib import Path
from typing import Optional
import numpy as np

def float_to_fixed(value: float, shift: int = 16) -> int:
    """
    Convert floating-point value to Q16.16 fixed-point integer.

    Args:
        value: Float value to convert
        shift: Number of fractional bits (default: 16 for Q16.16)

    Returns:
        Fixed-point integer representation

    Example:
        >>> float_to_fixed(1.5)
        98304  # 1.5 * 2^16
    """
    return int(round(value * (1 << shift)))

def validate_range(value: float, min_val: float = -32768.0, max_val: float = 32767.99998) -> bool:
    """
    Validate that float value fits in Q16.16 range.

    Args:
        value: Float value to check
        min_val: Minimum Q16.16 value
        max_val: Maximum Q16.16 value

    Returns:
        True if value in range, False otherwise
    """
    return min_val <= value <= max_val

def quantize_array(arr: np.ndarray, name: str) -> tuple[list[int], int, int]:
    """
    Quantize numpy array to fixed-point, with range validation.

    Args:
        arr: Numpy array of float values
        name: Name for error reporting

    Returns:
        Tuple of (quantized_values, out_of_range_count, total_count)

    Raises:
        ValueError: If too many values out of range
    """
    flat = arr.flatten()
    quantized = []
    out_of_range = 0

    for i, val in enumerate(flat):
        if not validate_range(val):
            out_of_range += 1
            # Clamp to valid range
            val = max(-32768.0, min(32767.99998, val))
            print(f"Warning: {name}[{i}] = {flat[i]:.6f} out of range, clamped to {val:.6f}")

        quantized.append(float_to_fixed(val))

    if out_of_range > len(flat) * 0.01:  # More than 1% out of range
        raise ValueError(f"{name}: {out_of_range}/{len(flat)} values out of range. "
                        "Consider rescaling your model.")

    return quantized, out_of_range, len(flat)

def format_c_array(values: list[int], indent: str = "    ", values_per_line: int = 8) -> str:
    """
    Format fixed-point values as C array initialization.

    Args:
        values: List of fixed-point integers
        indent: Indentation string
        values_per_line: Number of values per line

    Returns:
        Formatted C array string
    """
    lines = []
    for i in range(0, len(values), values_per_line):
        chunk = values[i:i+values_per_line]
        line = indent + ", ".join(f"{v:10d}" for v in chunk)
        if i + values_per_line < len(values):
            line += ","
        lines.append(line)
    return "\n".join(lines)

def export_to_c_header(
    layer_name: str,
    weights: np.ndarray,
    bias: Optional[np.ndarray],
    output_path: Path,
    add_dimensions: bool = True
) -> dict:
    """
    Generate C header file with fixed-point weights and bias.

    Args:
        layer_name: Name for the layer (used in variable names)
        weights: Weight matrix as numpy array
        bias: Bias vector as numpy array (optional)
        output_path: Path to output .h file
        add_dimensions: Include dimension constants

    Returns:
        Dictionary with quantization statistics
    """
    stats = {
        'layer_name': layer_name,
        'weights_shape': weights.shape,
        'weights_count': weights.size,
        'weights_out_of_range': 0,
        'bias_count': 0,
        'bias_out_of_range': 0
    }

    # Quantize weights
    print(f"Quantizing weights: {weights.shape}")
    weight_fixed, w_oor, w_total = quantize_array(weights, f"{layer_name}_weights")
    stats['weights_out_of_range'] = w_oor

    # Quantize bias if provided
    bias_fixed = None
    if bias is not None:
        print(f"Quantizing bias: {bias.shape}")
        bias_fixed, b_oor, b_total = quantize_array(bias, f"{layer_name}_bias")
        stats['bias_count'] = b_total
        stats['bias_out_of_range'] = b_oor

    # Generate header file
    with open(output_path, 'w') as f:
        # Header guard
        guard = f"{layer_name.upper()}_WEIGHTS_H"
        f.write(f"/**\n")
        f.write(f" * @file {output_path.name}\n")
        f.write(f" * @brief Quantized weights for {layer_name} layer\n")
        f.write(f" * \n")
        f.write(f" * Automatically generated by SpeyTech Quantizer\n")
        f.write(f" * DO NOT EDIT MANUALLY\n")
        f.write(f" * \n")
        f.write(f" * Original shapes:\n")
        f.write(f" *   Weights: {weights.shape}\n")
        if bias is not None:
            f.write(f" *   Bias: {bias.shape}\n")
        f.write(f" * \n")
        f.write(f" * Quantization: Q16.16 fixed-point\n")
        f.write(f" */\n\n")

        f.write(f"#ifndef {guard}\n")
        f.write(f"#define {guard}\n\n")
        f.write(f'#include "fixed_point.h"\n\n')

        # Dimension constants
        if add_dimensions and len(weights.shape) == 2:
            f.write(f"/* Layer dimensions */\n")
            f.write(f"#define {layer_name.upper()}_INPUT_DIM  {weights.shape[0]}\n")
            f.write(f"#define {layer_name.upper()}_OUTPUT_DIM {weights.shape[1]}\n\n")

        # Weights array
        f.write(f"/* Weights: {weights.shape} = {weights.size} elements */\n")
        f.write(f"static const fixed_t {layer_name}_weights[{weights.size}] = {{\n")
        f.write(format_c_array(weight_fixed))
        f.write(f"\n}};\n\n")

        # Bias array
        if bias_fixed is not None:
            f.write(f"/* Bias: {bias.shape} = {bias.size} elements */\n")
            f.write(f"static const fixed_t {layer_name}_bias[{bias.size}] = {{\n")
            f.write(format_c_array(bias_fixed))
            f.write(f"\n}};\n\n")

        f.write(f"#endif /* {guard} */\n")

    return stats

def main():
    parser = argparse.ArgumentParser(
        description='SpeyTech Model Quantizer - Convert PyTorch weights to Q16.16 C headers',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quantize from NumPy arrays
  python quantize.py weights.npy bias.npy layer1 output/

  # Quantize from PyTorch checkpoint
  python quantize.py --torch model.pth layer1 output/

For commercial licensing and support: william@fstopify.com
        """
    )

    parser.add_argument('weights', type=str, help='Path to weights file (.npy or .pth)')
    parser.add_argument('layer_name', type=str, help='Name for the layer')
    parser.add_argument('output_dir', type=str, help='Output directory for .h file')
    parser.add_argument('--bias', type=str, help='Path to bias file (optional)')
    parser.add_argument('--torch', action='store_true', help='Input is PyTorch checkpoint')
    parser.add_argument('--no-dims', action='store_true', help='Skip dimension constants')

    args = parser.parse_args()

    # Load weights
    if args.torch:
        try:
            import torch
            checkpoint = torch.load(args.weights, map_location='cpu')
            # Assume weights are in checkpoint['weights'] or checkpoint
            if isinstance(checkpoint, dict) and 'weights' in checkpoint:
                weights = checkpoint['weights'].numpy()
            else:
                weights = checkpoint.numpy() if hasattr(checkpoint, 'numpy') else checkpoint
        except ImportError:
            print("Error: PyTorch not installed. Install with: pip install torch")
            return 1
        except Exception as e:
            print(f"Error loading PyTorch checkpoint: {e}")
            return 1
    else:
        try:
            weights = np.load(args.weights)
        except Exception as e:
            print(f"Error loading weights: {e}")
            return 1

    # Load bias if provided
    bias = None
    if args.bias:
        try:
            bias = np.load(args.bias)
        except Exception as e:
            print(f"Error loading bias: {e}")
            return 1

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    output_path = output_dir / f"{args.layer_name}_weights.h"

    print(f"\nSpeyTech Model Quantizer")
    print(f"{'='*50}")

    try:
        stats = export_to_c_header(
            args.layer_name,
            weights,
            bias,
            output_path,
            add_dimensions=not args.no_dims
        )

        print(f"\n✅ Quantization complete!")
        print(f"   Output: {output_path}")
        print(f"   Weights: {stats['weights_count']} values")
        if stats['weights_out_of_range'] > 0:
            print(f"   ⚠️  {stats['weights_out_of_range']} weight(s) clamped to Q16.16 range")
        if stats['bias_count'] > 0:
            print(f"   Bias: {stats['bias_count']} values")
            if stats['bias_out_of_range'] > 0:
                print(f"   ⚠️  {stats['bias_out_of_range']} bias value(s) clamped to Q16.16 range")

        print(f"\nInclude in your C code:")
        print(f'   #include "{output_path.name}"')

        return 0

    except Exception as e:
        print(f"\n❌ Error: {e}")
        return 1

if __name__ == '__main__':
    sys.exit(main())
